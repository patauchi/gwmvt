---
title: "GWPCA Robusto: Fundamentos Matemáticos y Algoritmos Detallados"
author: "Análisis Espacial Robusto"
format:
  html
---

# Introducción

El Análisis de Componentes Principales Geográficamente Ponderado Robusto (Robust GWPCA) extiende el GWPCA clásico incorporando estimadores robustos que minimizan el efecto de valores atípicos en los análisis espaciales. Este documento proporciona una descripción matemática completa y algorítmica de cada método implementado.

## ¿Qué es y por qué importa?

- PCA resume la variabilidad multivariada en pocas direcciones ortogonales (componentes) que maximizan varianza.
- GWPCA repite esa idea localmente: alrededor de cada ubicación se estima una media y covarianza con pesos espaciales, y se calculan los componentes de ese vecindario.
- La versión robusta sustituye media/covarianza clásicos por alternativas con menor sensibilidad a outliers, evitando que pocas observaciones anómalas dominen la estructura local.

## Resumen intuitivo (en 1 minuto)

1) Cerca de cada punto del mapa, ponderamos más a los vecinos cercanos (kernel + ancho de banda). 2) Con esos datos locales y pesos, estimamos centro y dispersión con un método robusto (MCD, S/MM, Huber, ROBPCA, etc.). 3) Descomponemos la covarianza local para obtener componentes principales, sus scores y varianzas explicadas. 4) Repetimos en todas las ubicaciones para mapear cómo cambia la estructura multivariada en el espacio, con menor impacto de outliers.

## Motivación

En análisis espaciales, los datos pueden contener:
- **Outliers globales**: Observaciones extremas en el espacio multivariado
- **Outliers locales**: Observaciones anómalas en contextos espaciales específicos
- **Contaminación espacial**: Clusters de observaciones atípicas

La robustez es crucial para obtener estimaciones confiables de la estructura de covarianza local.

## Cómo se comporta el estimador (visión general)

- En presencia de outliers aislados, los estimadores robustos mantienen estables la media/covarianza locales; los valores propios y loadings varían poco respecto a zonas "limpias".
- Con contaminación moderada (10–30%), métodos de alto punto de ruptura (MCD/S/MM) preservan la orientación de los principales componentes, a costa de algo de eficiencia cuando no hay outliers.
- El ancho de banda regula un segundo equilibrio: ventanas pequeñas capturan heterogeneidad local pero son más variables; ventanas grandes suavizan más y reducen varianza, pero pueden mezclar patrones distintos.
- En regiones con clusters de outliers, enfoques tipo MCD/MM y ROBPCA suelen ser más estables que M-estimadores simples (p. ej., Huber) si no se acompañan de una buena inicialización.

# GWPCA Estándar: Fundamentos

## Definición del Problema

Dado un conjunto de datos multivariados espaciales:
- Observaciones: $$\mathbf{X} = \{x_1, x_2, ..., x_n\}$$ donde $$x_k \in \mathbb{R}^p$$
- Ubicaciones espaciales: $$\mathbf{S} = \{s_1, s_2, ..., s_n\}$$ donde $$s_k \in \mathbb{R}^2$$
- Objetivo: Obtener componentes principales locales en cada ubicación $$s_i$$

## Algoritmo del GWPCA Estándar

### Intuición rápida

- Damos más peso a las observaciones cercanas a la ubicación focal.
- Calculamos una media y covarianza locales ponderadas y hacemos PCA sobre esa covarianza.
- Al desplazar la ubicación focal por el mapa, los componentes y su varianza explicada cambian suavemente.

### Paso 1: Cálculo de Pesos Espaciales

Para cada ubicación focal $$s_i$$, calculamos los pesos espaciales usando una función kernel:

**Kernel Gaussiano (más común):**
$$w_k(s_i) = \exp\left(-\frac{d(s_i, s_k)^2}{2h^2}\right)$$

**Kernel Bisquare:**
$$w_k(s_i) = \begin{cases}
\left(1 - \left(\frac{d(s_i, s_k)}{h}\right)^2\right)^2 & \text{si } d(s_i, s_k) < h \\
0 & \text{en otro caso}
\end{cases}$$

donde:
- $$d(s_i, s_k)$$ es la distancia euclidiana entre ubicaciones
- $$h$$ es el ancho de banda (bandwidth)

Notas prácticas:
- Anchos de banda adaptativos (k vecinos más cercanos) tienden a ser más estables cuando la densidad de puntos varía en el espacio.
- Kernels compactos (bisquare) limitan la influencia a un radio finito; kernels suaves (gaussiano) evitan discontinuidades en bordes.

### Paso 2: Estimación de la Media Local Ponderada

La media local en la ubicación $$s_i$$ se calcula como:

$$\bar{x}(s_i) = \frac{\sum_{k=1}^{n} w_k(s_i) \cdot x_k}{\sum_{k=1}^{n} w_k(s_i)}$$

**Algoritmo:**
```
1. Inicializar: media_local = vector_cero(p)
2. suma_pesos = 0
3. Para cada observación k = 1 to n:
   a. Calcular peso w_k(s_i)
   b. media_local += w_k(s_i) * x_k
   c. suma_pesos += w_k(s_i)
4. media_local = media_local / suma_pesos
```

### Paso 3: Estimación de la Matriz de Covarianza Local

La matriz de covarianza ponderada en $$s_i$$:

$$\Sigma(s_i) = \frac{\sum_{k=1}^{n} w_k(s_i) \cdot (x_k - \bar{x}(s_i))(x_k - \bar{x}(s_i))^T}{\sum_{k=1}^{n} w_k(s_i)}$$

**Algoritmo:**
```
1. Centrar los datos: z_k = x_k - media_local
2. Inicializar: Sigma = matriz_cero(p, p)
3. Para cada observación k = 1 to n:
   a. Sigma += w_k(s_i) * z_k * z_k^T
4. Sigma = Sigma / suma_pesos
```

### Paso 4: Descomposición en Valores Propios

$$\Sigma(s_i) \cdot \phi_j(s_i) = \lambda_j(s_i) \cdot \phi_j(s_i)$$

donde:
- $$\lambda_j(s_i)$$: j-ésimo valor propio (varianza del j-ésimo componente)
- $$\phi_j(s_i)$$: j-ésimo vector propio (loadings del j-ésimo componente)

### Paso 5: Cálculo de Scores Locales

Para cada observación $$x_k$$, el score en el j-ésimo componente:

$$score_{jk}(s_i) = (x_k - \bar{x}(s_i))^T \cdot \phi_j(s_i)$$

Interpretación: los scores son las coordenadas de cada observación respecto a las direcciones principales locales. Mapear su varianza o los loadings permite entender qué variables "explican" cada componente en cada zona.

### Formulación matemática y conexión con el algoritmo

- Rayleigh: los loadings locales \(\phi_j(s_i)\) maximizan \(R(\phi)= \phi^T\Sigma(s_i)\phi\) sujeto a \(\|\phi\|=1\). La solución son autovectores de \(\Sigma(s_i)\).
- Reconstrucción: el error cuadrático kernel-ponderado \(\sum_k w_k(s_i)\|x_k-\bar{x}(s_i)-P_m(x_k-\bar{x}(s_i))\|^2\) se minimiza con el subespacio generado por los m autovectores principales locales.
- Local likelihood: \(\Sigma(s_i)\) es el estimador de segundo momento de máxima verosimilitud bajo Normalidad con pesos kernel fijos; la versión robusta reemplaza dicho estimador por su análogo robusto.

# GWPCA Robusto: Marco Teórico

## Principio General

El GWPCA Robusto reemplaza los estimadores clásicos por estimadores robustos:

$$(\tilde{\mu}(s_i), \tilde{\Sigma}(s_i)) = \mathcal{R}\left(\{(x_k, w_k(s_i))\}_{k=1}^n\right)$$

donde $$\mathcal{R}$$ es un procedimiento de estimación robusta que debe:
1. Incorporar los pesos espaciales $$w_k(s_i)$$
2. Ser resistente a outliers
3. Mantener propiedades deseables (equivarianza afín, eficiencia)

### Qué cambia respecto al GWPCA clásico

- Reemplazamos la media/covarianza locales por versiones robustas que reducen la influencia de observaciones extremas.
- Los pesos espaciales siguen presentes; se combinan con pesos de robustez (derivados de distancias robustas, LOF, profundidad, etc.).
- El resultado es un conjunto de componentes locales menos sensibles a contaminación, con un balance sesgo–varianza controlado por el método y sus parámetros.

### Implementación genérica con pesos espaciales

Dado un método robusto que define pesos de robustez \(w^{rob}_k\) a partir de distancias robustas, la combinación natural es \(w^{tot}_k = w^{esp}_k\, w^{rob}_k\). Las ecuaciones de estimación toman la forma
\[\hat{\mu} = \frac{\sum_k w^{tot}_k x_k}{\sum_k w^{tot}_k},\qquad \hat{\Sigma} \propto \sum_k w^{tot}_k (x_k-\hat{\mu})(x_k-\hat{\mu})^T,\]
con factores de consistencia para que, bajo Normalidad y sin outliers, \(E[\hat{\Sigma}] = \Sigma\). Esta receta traduce directamente las formulaciones matemáticas a implementaciones por ventana espacial.

### Comportamiento del estimador

En términos prácticos, los estimadores robustos de centro y dispersión local atenúan la influencia de observaciones extremas, de modo que los valores propios y las direcciones principales varían de forma suave y coherente con el patrón dominante. Ante contaminación baja, su rendimiento es cercano al clásico (ligero aumento de varianza); ante contaminación moderada/alta, evitan sesgos fuertes y rotaciones erráticas de los componentes. La ventana espacial añade un segundo efecto de suavizado: a menor ancho de banda, más detalle local pero más variabilidad; a mayor ancho de banda, menor varianza pero mayor mezcla de estructuras vecinas. Elegir el método (MCD/S/MM/Huber/ROBPCA) y sus parámetros ajusta explícitamente este compromiso.

## Medidas de Robustez

### Punto de Ruptura (Breakdown Point)

El punto de ruptura $$\epsilon^*$$ es la proporción máxima de contaminación que un estimador puede tolerar:

$$\epsilon^* = \max\left\{\epsilon : \sup_{X_\epsilon} \|\hat{\theta}(X_\epsilon) - \theta_0\| < \infty\right\}$$

### Función de Influencia

Mide la sensibilidad del estimador a una observación individual:

$$IF(x; \hat{\theta}, F) = \lim_{\epsilon \to 0} \frac{\hat{\theta}((1-\epsilon)F + \epsilon\delta_x) - \hat{\theta}(F)}{\epsilon}$$

Comentario práctico: funciones de influencia acotadas implican que el impacto de una observación individual no crece sin límite. Puntos de ruptura altos (~50%) permiten tolerar grandes proporciones de contaminación, pero a menudo con menor eficiencia cuando los datos son limpios.

### Equivarianza afín

Un estimador \(T\) de (μ, Σ) es equivariante afín si para cualquier transformación no singular \(A\) y vector \(b\): \(T(AX+b) = (A\,\hat{\mu}+b,\; A\,\hat{\Sigma}\,A^T)\). Esta propiedad garantiza que los componentes principales (direcciones y varianzas) se transforman correctamente bajo cambios lineales de escala, rotación y sesgo, algo esencial en PCA/GWPCA.

# Estimadores Robustos Detallados

## 3.1. M-Estimador de Huber

### Fundamento Teórico

Los M-estimadores minimizan una función objetivo robusta:

$$(\hat{\mu}, \hat{\Sigma}) = \arg\min_{\mu, \Sigma} \sum_{k=1}^n w_k(s_i) \cdot \rho\left(\frac{\|x_k - \mu\|_{\Sigma^{-1}}}{\sigma}\right)$$

donde $$\rho$$ es la función de pérdida de Huber:

$$\rho_c(u) = \begin{cases}
\frac{1}{2}u^2 & \text{si } |u| \leq c \\
c|u| - \frac{1}{2}c^2 & \text{si } |u| > c
\end{cases}$$

#### Ecuaciones de estimación (modelo elíptico)

Bajo un modelo elíptico para X con centro μ y dispersión Σ, y distancias robustas \(d_k^2 = (x_k-\mu)^T\Sigma^{-1}(x_k-\mu)\), las condiciones de primer orden implican ecuaciones de tipo ponderado:

- Para la localización: \(\sum_k w_k^{esp} \, \omega(d_k/\sigma)\,(x_k-\mu) = 0\), con \(\omega(t)= \psi(t)/t\).
- Para la dispersión: \(\Sigma = \frac{1}{\beta} \sum_k w_k^{esp} \, u(d_k/\sigma)\,(x_k-\mu)(x_k-\mu)^T\), con \(u(t) = \min(1, c^2/t^2)\) en Huber, y \(\beta\) factor de consistencia para Normalidad (corrige sesgo: \(\beta=E[u(\chi^2_p)]\)).

Estas ecuaciones llevan directamente a un esquema IRWLS donde \(\omega\) y \(u\) definen los pesos iterativos.

### Función de Influencia (ψ-función)

La derivada de $$\rho$$ define la función de influencia:

$$\psi_c(u) = \frac{\partial \rho_c(u)}{\partial u} = \begin{cases}
u & \text{si } |u| \leq c \\
c \cdot \text{sign}(u) & \text{si } |u| > c
\end{cases}$$

Intuición y comportamiento:
- Para residuos pequeños actúa como PCA clásico (cuadrático); para grandes, recorta la influencia a un nivel constante.
- Mejora la estabilidad ante unos pocos outliers, pero su punto de ruptura es bajo si se usa sin una inicialización robusta de escala/centro.
- Recomendable con baja–moderada contaminación o como refinamiento tras un arranque robusto (MCD/S).

#### Propiedades
- Equivarianza afín: sí, si las ecuaciones se formulan para localización y dispersión conjuntamente.
- Punto de ruptura: bajo sin inicialización robusta; mejora con arranques MCD/S.
- IF: acotada (por la forma lineal–constante de ψ de Huber), lo que limita la influencia individual.
- Eficiencia asintótica: ajustable con c; típicamente c=1.345 ≈ 95% bajo Normal.

#### De las matemáticas al algoritmo (con pesos espaciales)
- Reemplace sumas \(\sum\) por sumas ponderadas \(\sum w_k^{esp}\).
- Use \(\omega\) para actualizar μ y \(u\) para actualizar Σ; incluya \(\beta\) para consistencia.
- Regularice Σ si es necesario (ridge/shrinkage) antes de descomponer en autovalores.

### Algoritmo IRWLS (Iteratively Reweighted Least Squares)

**Paso 1: Inicialización**
```
1. Estimar media inicial: μ⁰ = mediana_espacial_ponderada(X, w)
2. Estimar Σ⁰ = MAD_ponderado(X, w) * I_p
3. Fijar c = 1.345 (95% eficiencia en normal)
4. contador = 0, ε = 10^-6
```

**Paso 2: Iteración hasta convergencia**
```
Repetir hasta convergencia:
  1. Calcular distancias de Mahalanobis:
     d_k = sqrt((x_k - μ^t)^T * (Σ^t)^(-1) * (x_k - μ^t))
  
  2. Calcular pesos de Huber:
     v_k = ψ_c(d_k) / d_k = min(1, c/d_k)
  
  3. Actualizar media:
     μ^(t+1) = Σ(w_k * v_k * x_k) / Σ(w_k * v_k)
  
  4. Actualizar covarianza:
     Σ^(t+1) = Σ(w_k * v_k * (x_k - μ^(t+1))(x_k - μ^(t+1))^T) / Σ(w_k)
  
  5. Verificar convergencia:
     Si ||μ^(t+1) - μ^t|| < ε y ||Σ^(t+1) - Σ^t||_F < ε:
        Terminar
  
  6. t = t + 1
```

## 3.2. Determinante de Covarianza Mínima (MCD)

### Fundamento Teórico

El MCD busca el subconjunto $$H \subset \{1,...,n\}$$ con $$|H| = h$$ que minimiza:

$$\det\left(\Sigma_H\right)$$

donde $$\Sigma_H$$ es la matriz de covarianza del subconjunto $$H$$.

#### Parámetros y propiedades

- Tamaño del subconjunto: $$h = \lfloor(n+p+1)/2\rfloor$$ (máximo punto de ruptura)
- Punto de ruptura: $$\epsilon^* = (n-h)/n \approx 0.5$$
- Equivarianza afín: sí (aplica a transformaciones lineales no singulares).
- Consistencia: requiere factores de corrección por muestra finita y por Normalidad.
- Eficiencia: mejora mediante el paso de reponderación (reweighted MCD).

### Algoritmo Fast-MCD

Intuición y comportamiento:
- Busca el subconjunto de h observaciones más "concentradas" y estima media/covarianza con ellas.
- Alto punto de ruptura (≈50%) y buena detección de outliers, incluso en clusters; menor eficiencia en datos perfectamente gaussianos.
- Muy útil como estimación inicial para MM o como base en ventanas con contaminación heterogénea.

**Paso 1: Inicialización con Múltiples Arranques**
```
Para j = 1 to n_starts (típicamente 500):
  1. Seleccionar aleatoriamente p+1 observaciones
  2. Calcular μ₀ y Σ₀ de este subconjunto
  3. Si det(Σ₀) = 0, reintentar
  4. Guardar como candidato inicial
```

**Paso 2: C-Steps (Concentration Steps)**
```
Para cada candidato inicial:
  Repetir hasta convergencia:
    1. Calcular distancias de Mahalanobis:
       d²_k = (x_k - μ_old)^T * Σ_old^(-1) * (x_k - μ_old)
    
    2. Ordenar: d²_(1) ≤ d²_(2) ≤ ... ≤ d²_(n)
    
    3. Seleccionar H = {índices de las h menores distancias}
    
    4. Actualizar estimadores con ponderación espacial:
       μ_new = Σ(k∈H) w_k * x_k / Σ(k∈H) w_k
       Σ_new = Σ(k∈H) w_k * (x_k - μ_new)(x_k - μ_new)^T / Σ(k∈H) w_k
    
    5. Si det(Σ_new) ≥ det(Σ_old):
       Convergencia alcanzada
    
    6. μ_old = μ_new, Σ_old = Σ_new
```

**Paso 3: Refinamiento**
```
1. Seleccionar el mejor resultado (menor determinante)
2. Aplicar paso de reponderación:
   a. Calcular d²_k con estimadores finales
   b. Definir pesos: w_rob,k = 1 si d²_k ≤ χ²_p,0.975, 0 sino
   c. Recalcular μ y Σ con estos pesos
```

#### De las matemáticas al algoritmo
- Problema discreto: minimizar det(Σ_H) sobre subconjuntos de tamaño h; se aproxima con C-steps que garantizan no empeorar el determinante.
- Pesos espaciales: dentro de H, reemplace medias y covarianzas por versiones ponderadas por \(w_k^{esp}\).
- Consistencia: aplique factor de escala \(c_{n,p,h}\) para que E[Σ] ≈ Σ_0 bajo Normal.

## 3.3. Elipsoide de Volumen Mínimo (MVE)

### Fundamento Teórico

El MVE encuentra el elipsoide de mínimo volumen que contiene al menos h observaciones:

$$\min_{\mu, \Sigma} \det(\Sigma) \text{ sujeto a } |\{k : (x_k-\mu)^T\Sigma^{-1}(x_k-\mu) \leq c^2\}| \geq h$$

#### Propiedades y relación con MCD

$$\text{MVE}(\Sigma) = c_p \cdot \text{MCD}(\Sigma)$$

donde $$c_p$$ es un factor de corrección dimensional.

- Alto punto de ruptura (≈50%) pero menor eficiencia que MCD.
- Suele usarse como arranque; el reweighting es clave para eficiencia.

### Algoritmo de Búsqueda Aleatoria

**Paso 1: Muestreo Exhaustivo**
```
Para j = 1 to n_samples (típicamente 3000):
  1. Seleccionar aleatoriamente p+1 puntos
  2. Calcular el elipsoide mínimo que los contiene:
     a. Centro: μ_j = media(puntos seleccionados)
     b. Forma: Σ_j = cov(puntos seleccionados)
  
  3. Escalar el elipsoide hasta contener h puntos:
     a. Calcular todas las distancias d²_k
     b. Encontrar c_j tal que exactamente h puntos cumplan d²_k ≤ c²_j
     c. Σ_j = c²_j * Σ_j
  
  4. Calcular volumen: V_j = det(Σ_j)^(1/2)
```

**Paso 2: Selección y Refinamiento**
```
1. Seleccionar j* = argmin_j V_j
2. μ_MVE = μ_j*, Σ_MVE = Σ_j*
3. Aplicar corrección de sesgo finito:
   Σ_MVE = κ_p,n * Σ_MVE
   donde κ_p,n es el factor de corrección pequeña muestra
```

#### De las matemáticas al algoritmo
- Se busca el elipsoide de mínimo volumen que cubra al menos h puntos; en la práctica se exploran muestras pequeñas (p+1) y se expanden para cubrir h observaciones por distancia elíptica.
- Ponderación espacial: compute distancias y conteos usando \(w_k^{esp}\) como multiplicadores en medias/covarianzas.

## 3.4. S-Estimadores

### Fundamento Teórico

Los S-estimadores minimizan una medida de dispersión robusta:

$$(\hat{\mu}, \hat{\Sigma}) = \arg\min_{\mu,\Sigma} s(\{d_k(\mu,\Sigma)\}_{k=1}^n)$$

donde $$s$$ satisface:

$$\frac{1}{n} \sum_{k=1}^n \rho\left(\frac{d_k}{s}\right) = b_0$$

con $$b_0 = E_{\Phi}[\rho(||Z||)]$$ para $$Z \sim N(0,I_p)$$.

#### Ecuaciones de estimación
- Dadas \(s\) y \(\rho\), los pesos \(w_k = \psi(d_k/s)/(d_k/s)\) inducen:
  - \(\sum_k w_k^{esp} w_k (x_k-\mu) = 0\)
  - \(\Sigma \propto \sum_k w_k^{esp} w_k (x_k-\mu)(x_k-\mu)^T\) con restricción \(|\Sigma|=1\) o ajuste de escala separado.
- La escala s se determina resolviendo la ecuación de normalización de \(\rho\) para alcanzar el valor objetivo \(b_0\) (consistencia y breakdown deseado).

### Función ρ de Tukey (Bisquare)

$$\rho_c(u) = \begin{cases}
\frac{u^2}{2} - \frac{u^4}{2c^2} + \frac{u^6}{6c^4} & \text{si } |u| \leq c \\
\frac{c^2}{6} & \text{si } |u| > c
\end{cases}$$

### Algoritmo de Ruppert

**Paso 1: Inicialización con MCD**
```
1. Obtener (μ₀, Σ₀) = MCD(X, h=⌊0.5n⌋)
2. Fijar c = 1.548 (50% punto de ruptura)
3. Calcular b₀ = 0.5 (para 50% punto de ruptura)
```

**Paso 2: Iteración para la Escala**
```
Repetir hasta convergencia:
  1. Calcular distancias: d_k = ||x_k - μ||_Σ
  
  2. Resolver para s:
     (1/n) Σ ρ(d_k/s) = b₀
     Usando Newton-Raphson:
     s^(t+1) = s^t - [f(s^t) - b₀] / f'(s^t)
     donde f(s) = (1/n) Σ ρ(d_k/s)
  
  3. Calcular pesos:
     w_k = ψ(d_k/s) / (d_k/s)
     donde ψ es la derivada de ρ
  
  4. Actualizar estimadores con ponderación espacial:
     μ = Σ(w_k * w_espacial_k * x_k) / Σ(w_k * w_espacial_k)
     Σ = s² * Σ(w_k * w_espacial_k * u_k * u_k^T) / Σ(w_k * w_espacial_k)
     donde u_k = (x_k - μ) / s
```

#### Propiedades
- Equivarianza afín: sí.
- Punto de ruptura: hasta 50% con \(\rho\) bisquare y elección adecuada de \(b_0\).
- IF: acotada (ψ redescendiente), robustez elevada ante outliers severos.
- Eficiencia: depende de c; típicamente c≈1.548 para alto breakdown, pudiendo refinarse luego (MM).

## 3.5. MM-Estimadores

### Fundamento Teórico

Proceso en dos etapas que combina robustez y eficiencia:

1. **Etapa S**: Obtener estimador inicial de alto punto de ruptura
2. **Etapa M**: Refinar para mayor eficiencia manteniendo la escala fija

### Funciones ρ Diferentes

- **Etapa S**: $$\rho_1$$ con $$c_1 = 1.548$$ (50% punto de ruptura)
- **Etapa M**: $$\rho_2$$ con $$c_2 = 4.685$$ (95% eficiencia)

### Algoritmo MM Completo

Intuición y comportamiento:
- Hereda el alto punto de ruptura de la etapa S y logra alta eficiencia con la etapa M.
- Estable frente a outliers moderados/altos, manteniendo buena precisión cuando los datos son casi gaussianos.
- Suele ser la opción por defecto cuando el costo computacional es aceptable.

#### Ecuaciones de estimación (etapa M con escala fija)
- Con \(s_{final}\) fijo y ψ2, los pesos \(w_k = \psi_2(d_k/s_{final})/(d_k/s_{final})\) definen:
  - \(\sum_k w_k^{esp} w_k (x_k-\mu)=0\)
  - \(\Sigma \propto \sum_k w_k^{esp} v_k (x_k-\mu)(x_k-\mu)^T\), con \(v_k = w_k [\psi_2(d_k/s_{final})]^2\) u otros esquemas equivalentes para consistencia.

#### Propiedades
- Equivarianza afín: sí.
- Punto de ruptura: el de la etapa S (hasta 50%).
- Eficiencia: ajustable con \(c_2\) (p. ej., 4.685 ≈ 95% bajo Normal).

**Etapa 1: S-Estimación**
```
1. Obtener (μ_S, Σ_S, s_S) usando S-estimador
2. Fijar la escala: s_final = s_S
```

**Etapa 2: M-Estimación con Escala Fija**
```
Inicializar: μ⁰ = μ_S, Σ⁰ = Σ_S
Repetir hasta convergencia:
  1. Calcular distancias escaladas:
     d_k = ||x_k - μ^t||_Σ^t / s_final
  
  2. Calcular pesos MM:
     w_k = ψ₂(d_k) / d_k
     donde ψ₂ usa c₂ = 4.685
  
  3. Actualizar con ponderación espacial:
     μ^(t+1) = Σ(w_k * w_espacial_k * x_k) / Σ(w_k * w_espacial_k)
     
  4. Para la covarianza, usar pesos modificados:
     v_k = w_k * [ψ₂(d_k)]²
     Σ^(t+1) = Σ(v_k * w_espacial_k * (x_k - μ^(t+1))(x_k - μ^(t+1))^T) / Σ(v_k * w_espacial_k)
  
  5. Verificar convergencia
```

## 3.6. BACON (Blocked Adaptive Computationally-efficient Outlier Nominators)

### Fundamento Teórico

BACON construye iterativamente un subconjunto "limpio" comenzando desde un núcleo inicial robusto.

#### Propiedades
- No es estrictamente equivariante afín a menos que se estandaricen las variables (recomendable: robust-z por mediana/MAD).
- Breakdown efectivo depende del proceso de expansión; tiende a ser menor que 50% pero escala bien en n.
- Eficiencia alta computacionalmente; útil como filtro previo para luego estimar Σ en el subconjunto limpio.

### Algoritmo BACON Detallado

**Paso 1: Selección del Subconjunto Inicial**
```
1. Calcular distancias euclidianas al centroide:
   centroide = mediana_componente_wise(X)
   dist_k = ||x_k - centroide||₂
   
2. Seleccionar m = max(p+1, ⌊n*0.1⌋) observaciones con menor distancia
3. Conjunto inicial: B₀ = {índices de las m observaciones más cercanas}
```

**Paso 2: Expansión Iterativa**
```
t = 0
Repetir hasta que B no cambie:
  1. Calcular estimadores del subconjunto actual con pesos espaciales:
     μ_B = Σ(k∈B) w_k * x_k / Σ(k∈B) w_k
     Σ_B = Σ(k∈B) w_k * (x_k - μ_B)(x_k - μ_B)^T / Σ(k∈B) w_k
  
  2. Calcular distancias de Mahalanobis para TODOS los puntos:
     d²_k = (x_k - μ_B)^T * Σ_B^(-1) * (x_k - μ_B)
  
  3. Determinar umbral adaptativo:
     c_t = χ²_p,1-α * correction_factor(|B_t|, n, p)
     donde correction_factor = 1 + (p+1)/(n-|B_t|) + 2/(n-1-3p)
  
  4. Actualizar conjunto:
     B_(t+1) = {k : d²_k < c_t}
  
  5. Criterio de parada:
     Si |B_(t+1)| = |B_t| o |B_(t+1)| > n/2:
        Terminar
  
  6. t = t + 1
```

**Paso 3: Refinamiento Final**
```
1. μ_BACON = μ_B_final
2. Σ_BACON = factor_corrección * Σ_B_final
   donde factor_corrección compensa el sesgo por selección
```

#### De las matemáticas al algoritmo
- Se aproxima el conjunto H "limpio" donde las distancias de Mahalanobis (respecto a estimadores del propio H) siguen una \(\chi^2_p\) truncada; el umbral se ajusta por tamaño de H y dimensión p.
- Con pesos espaciales, las medias/covarianzas dentro de H se calculan ponderadas por \(w_k^{esp}\), y los umbrales pueden adaptarse con factores dependientes de |H|.

## 3.7. Local Outlier Factor (LOF)

### Fundamento Teórico

LOF mide la anomalía de un punto comparando su densidad local con la de sus vecinos:

$$LOF_k = \frac{1}{|N_k|} \sum_{j \in N_k} \frac{lrd(x_j)}{lrd(x_k)}$$

### Definiciones Clave

**k-distancia:**
$$d_k(x) = \text{distancia al k-ésimo vecino más cercano}$$

**Distancia de alcanzabilidad:**
$$reach\_dist_k(x, y) = \max(d_k(y), d(x,y))$$

**Densidad de alcanzabilidad local:**
$$lrd_k(x) = \left(\frac{1}{|N_k(x)|} \sum_{y \in N_k(x)} reach\_dist_k(x, y)\right)^{-1}$$

### Algoritmo LOF para GWPCA

Intuición y comportamiento:
- LOF no estima covarianza; mide rareza relativa de densidad. Se usa para transformar esa rareza en pesos de robustez.
- Funciona bien cuando los outliers son de baja densidad respecto a sus vecinos; puede penalizar regiones de borde si k es muy bajo o muy alto.
- Útil como capa previa de pesos combinados con el kernel espacial.

#### Propiedades e implementación
- No es equivariante afín; realizar estandarización previa es recomendable.
- Complejidad: O(n log n + n k log n) con estructuras de vecinos eficientes.
- Conversión matemática a pesos: \(w^{rob}_i=\exp(-\lambda\,\max(0, LOF_i-1))\) mantiene la continuidad y acota la influencia de altas razones de rareza.

**Paso 1: Cálculo de LOF**
```
Para cada punto x_i:
  1. Encontrar k-vecinos más cercanos N_k(x_i)
  2. Calcular k-distancia: d_k(x_i)
  3. Para cada vecino x_j ∈ N_k(x_i):
     Calcular reach_dist_k(x_i, x_j) = max(d_k(x_j), d(x_i, x_j))
  4. Calcular lrd_k(x_i) = |N_k(x_i)| / Σ reach_dist_k(x_i, x_j)
  5. Calcular LOF_k(x_i) = Σ(lrd_k(x_j)/lrd_k(x_i)) / |N_k(x_i)|
```

**Paso 2: Conversión a Pesos de Robustez**
```
1. Transformar LOF a pesos:
   w_rob,k = exp(-λ * max(0, LOF_k - 1))
   donde λ controla la agresividad (típicamente λ = 3)
   
2. Normalizar pesos:
   w_rob,k = w_rob,k / max(w_rob)
```

**Paso 3: Estimación Robusta Ponderada**
```
1. Combinar pesos espaciales y de robustez:
   w_total,k = w_espacial,k * w_rob,k
   
2. Calcular estimadores:
   μ_LOF = Σ(w_total,k * x_k) / Σ(w_total,k)
   Σ_LOF = Σ(w_total,k * (x_k - μ_LOF)(x_k - μ_LOF)^T) / Σ(w_total,k)
```

## 3.8. ROBPCA

### Fundamento Teórico

ROBPCA combina proyección-persecución (PP) con estimadores robustos en espacios de menor dimensión.

#### Distancias de diagnóstico
- Score Distance (SD): \(SD_i = \sqrt{(t_i-\mu_T)^T \Sigma_T^{-1} (t_i-\mu_T)}\)
- Orthogonal Distance (OD): \(OD_i = \|x_i - \hat{x}_i\|_2\), con \(\hat{x}_i\) la reconstrucción en el subespacio robusto.
- Umbrales típicos: SD comparada con \(\chi^2_k\) y OD con corte robusto (p. ej., Q3 + 1.5 IQR de OD).

### Algoritmo ROBPCA Completo

**Paso 1: Preprocesamiento y Reducción Inicial**
```
1. Centrado robusto inicial:
   μ₀ = L1-mediana(X) (mediana espacial)
   X_cent = X - μ₀
   
2. SVD inicial para reducción de dimensión:
   Si n < p:
     [U, S, V] = SVD(X_cent^T)
     X_red = X_cent * V[:,1:rank]
   Sino:
     X_red = X_cent
```

**Paso 2: Proyección Persecución (PP)**
```
Para j = 1 to k (número de componentes):
  1. Inicializar: v_j = vector aleatorio unitario
  
  2. Repetir hasta convergencia:
     a. Proyectar datos: z = X_red * v_j
     
     b. Calcular escala robusta:
        s_j = MAD(z) * 1.4826
     
     c. Calcular pesos:
        w_i = ψ(z_i/s_j) donde ψ es función de Huber
     
     d. Actualizar dirección con pesos espaciales:
        v_j = Σ(w_i * w_espacial_i * x_i * z_i) / ||...||
     
     e. Verificar convergencia:
        Si ||v_j^new - v_j^old|| < ε: terminar
  
  3. Deflación:
     X_red = X_red - z * v_j^T
```

**Paso 3: MCD en Subespacio**
```
1. Proyectar al subespacio k-dimensional:
   T = X_cent * P_k donde P_k = [v_1, ..., v_k]
   
2. Aplicar MCD en el subespacio:
   (μ_T, Σ_T) = MCD(T, h=⌊0.75n⌋)
   
3. Identificar outliers:
   d²_i = (t_i - μ_T)^T * Σ_T^(-1) * (t_i - μ_T)
   outliers = {i : d²_i > χ²_k,0.975}
```

**Paso 4: Re-estimación y Proyección Inversa**
```
1. Subset limpio: H = {i : i ∉ outliers}

2. Re-estimar en el subespacio con pesos espaciales:
   μ_clean = Σ(i∈H) w_i * t_i / Σ(i∈H) w_i
   Σ_clean = Σ(i∈H) w_i * (t_i - μ_clean)(t_i - μ_clean)^T / Σ(i∈H) w_i

3. Proyección inversa al espacio original:
   μ_ROBPCA = μ₀ + P_k * μ_clean
   Σ_ROBPCA = P_k * Σ_clean * P_k^T + σ²_residual * P_⊥ * P_⊥^T
   donde P_⊥ son las direcciones ortogonales
```

Intuición y comportamiento:
- Adecuado cuando p es grande y/o p > n: reduce dimensión de forma robusta antes de estimar la estructura.
- Estable frente a outliers de fila (observacionales) y, con deflación, frente a outliers esparsos en direcciones específicas.
- Suele capturar subespacios verdaderos con buena precisión incluso con contaminación moderada.

#### De las matemáticas al algoritmo
- La PP maximiza una medida robusta de dispersión de proyecciones (p. ej., MAD) para hallar direcciones; esto equivale a resolver problemas 1D robustos repetidamente y deflactar.
- En el subespacio k, estimar (μ_T, Σ_T) con MCD/S/MM; proyectar de vuelta para obtener (μ, Σ) en el espacio original.

## 3.9. Estimador Basado en Profundidad Espacial

### Fundamento Teórico

La profundidad espacial mide cuán "central" es un punto respecto a la distribución:

$$SD(y, X) = 1 - \left\|\frac{1}{n}\sum_{k=1}^n \frac{y - x_k}{\|y - x_k\|}\right\|$$

### Propiedades

- **Máximo en el centro**: $$SD(y, X) = 1$$ cuando $$y$$ es el centro espacial
- **Mínimo en el infinito**: $$SD(y, X) \to 0$$ cuando $$\|y\| \to \infty$$
- **Invarianza afín**: $$SD(Ay + b, AX + b) = SD(y, X)$$

### Algoritmo de Profundidad Espacial

**Paso 1: Cálculo de Profundidades**
```
Para cada observación x_i:
  1. Calcular vector de direcciones unitarias:
     u_ij = (x_i - x_j) / ||x_i - x_j|| para j ≠ i
  
  2. Calcular suma vectorial:
     S_i = (1/n) * Σ(j≠i) u_ij
  
  3. Profundidad espacial:
     SD_i = 1 - ||S_i||
  
  4. Manejar casos especiales:
     Si x_i = x_j para algún j: usar perturbación pequeña
```

**Paso 2: Transformación a Pesos**
```
1. Normalizar profundidades:
   SD_norm,i = SD_i / max(SD)
   
2. Aplicar función de transformación:
   w_depth,i = SD_norm,i^α
   donde α > 0 controla la influencia (típicamente α = 2)
```

**Paso 3: Estimación Ponderada**
```
1. Combinar con pesos espaciales:
   w_final,i = w_espacial,i * w_depth,i
   
2. Calcular estimadores:
   μ_SD = Σ(w_final,i * x_i) / Σ(w_final,i)
   Σ_SD = Σ(w_final,i * (x_i - μ_SD)(x_i - μ_SD)^T) / Σ(w_final,i)
```

### Mediana Espacial como Caso Especial

La mediana espacial minimiza:
$$\hat{\mu} = \arg\min_y \sum_{k=1}^n w_k \|y - x_k\|$$

**Algoritmo de Weiszfeld:**
```
Repetir hasta convergencia:
  μ^(t+1) = Σ(w_k * x_k / ||x_k - μ^t||) / Σ(w_k / ||x_k - μ^t||)
```

#### Estimadores depth-weighted
- Media y covarianza ponderadas por profundidad: \(\mu_D = \sum w^{esp}_i f(SD_i) x_i / \sum w^{esp}_i f(SD_i)\), \(\Sigma_D \propto \sum w^{esp}_i f(SD_i) (x_i-\mu_D)(x_i-\mu_D)^T\), con f creciente en la centralidad (p. ej., f(t)=t^\alpha).
- Propiedades: robustez controlada por f; la equivarianza depende del tipo de profundidad (la profundidad espacial es ortogonalmente invariante y, con transformaciones adecuadas, aproximadamente afín).

## 3.10. Recorte Espacial (Spatial Trimming)

### Fundamento Teórico

El recorte espacial elimina observaciones identificadas como outliers antes del cálculo final:

$$\hat{\theta}_{trim} = \theta_{classic}(X \setminus O)$$

donde $$O$$ es el conjunto de outliers detectados.

### Algoritmo de Recorte Espacial Adaptativo

**Paso 1: Detección Inicial de Outliers**
```
1. Obtener estimación robusta inicial:
   (μ_rob, Σ_rob) = MCD(X, h=⌊0.75n⌋)
   
2. Calcular distancias robustas:
   d²_rob,k = (x_k - μ_rob)^T * Σ_rob^(-1) * (x_k - μ_rob)
   
3. Factor de corrección para muestras finitas:
   c_n,p = (1 + 15/(n-p))^2
```

**Paso 2: Identificación Adaptativa**
```
1. Umbral adaptativo basado en la distribución empírica:
   q_emp = quantile(d²_rob, 1-α)
   q_teo = χ²_p,1-α
   c_adapt = max(q_emp, q_teo * c_n,p)
   
2. Clasificación con pesos espaciales:
   score_k = d²_rob,k / (w_espacial,k + ε)
   O = {k : score_k > c_adapt}
```

**Paso 3: Recorte Iterativo**
```
Repetir max_iter veces:
  1. Conjunto limpio: H = {1,...,n} \ O
  
  2. Re-estimar con pesos espaciales:
     μ_H = Σ(k∈H) w_k * x_k / Σ(k∈H) w_k
     Σ_H = Σ(k∈H) w_k * (x_k - μ_H)(x_k - μ_H)^T / Σ(k∈H) w_k
  
  3. Recalcular distancias:
     d²_k = (x_k - μ_H)^T * Σ_H^(-1) * (x_k - μ_H)
  
  4. Actualizar outliers con suavizado:
     O_new = {k : d²_k > c_adapt AND k ∈ O_prev}
     ∪ {k : d²_k > 1.5 * c_adapt}
  
  5. Criterio de convergencia:
     Si |O_new| = |O|: terminar
     O = O_new
```

**Paso 4: Estimación Final**
```
1. Aplicar factor de eficiencia:
   μ_trim = μ_H
   Σ_trim = c_eff * Σ_H
   donde c_eff = n / (n - |O|) * f_p
   y f_p es factor de corrección dimensional
```

#### Propiedades y puente teoría–implementación
- El trimming equivale a imponer pesos binarios 0/1 con un umbral sobre distancias robustas; el nivel de recorte α controla el breakdown teórico (≈α).
- En el marco espacial, el umbral puede depender de la ventana: combinar distancias robustas con \(w^{esp}\) evita descartar sistemáticamente zonas de baja densidad.

# Implementación Práctica y Consideraciones

## Regularización y Estabilidad Numérica

### Problema de Matrices Singulares

En contextos de alta dimensionalidad o muestras pequeñas locales, $$\tilde{\Sigma}$$ puede ser singular o mal condicionada.

### Soluciones Implementadas

**1. Simetrización Forzada:**
$$\tilde{\Sigma} \leftarrow \frac{1}{2}(\tilde{\Sigma} + \tilde{\Sigma}^T)$$

**2. Regularización Ridge:**
$$\tilde{\Sigma}_{reg} = \tilde{\Sigma} + \lambda I_p$$

donde $$\lambda$$ se elige adaptativamente:
$$\lambda = \max\left(10^{-6}, 0.01 \cdot \text{traza}(\tilde{\Sigma})/p\right)$$

**3. Regularización por Contracción (Shrinkage):**
$$\tilde{\Sigma}_{shrink} = \alpha \tilde{\Sigma} + (1-\alpha) \sigma^2 I_p$$

donde $$\alpha \in [0,1]$$ se estima por validación cruzada.

**4. Proyección al Cono de Matrices Definidas Positivas:**
```
1. Descomposición espectral: Σ = QΛQ^T
2. Truncamiento: Λ_+ = max(Λ, ε*I)
3. Reconstrucción: Σ_+ = Q*Λ_+*Q^T
```

## Selección del Método Robusto

### Criterios de Decisión

| Método | Punto de Ruptura | Eficiencia | Complejidad | Uso Recomendado |
|--------|------------------|------------|-------------|-----------------|
| Huber | 0% | 95% | O(np²) | Contaminación leve |
| MCD | 50% | 30-40% | O(np²log n) | Propósito general |
| MVE | 50% | 20-30% | O(n^p) | Muestras pequeñas |
| S-Est | 50% | 30% | O(np²) | Alto punto ruptura |
| MM-Est | 50% | 95% | O(np²) | Balance óptimo |
| BACON | 40% | 80% | O(np²) | Datasets grandes |
| LOF | Variable | Alta | O(n²p) | Outliers locales |
| ROBPCA | 50% | 60% | O(np²) | Alta dimensión |

### Diagrama de Flujo para Selección

```mermaid
graph TD
    A[Inicio] --> B{p > 10?}
    B -->|Sí| C[ROBPCA]
    B -->|No| D{n > 1000?}
    D -->|Sí| E[BACON]
    D -->|No| F{Contaminación esperada?}
    F -->|< 10%| G[Huber]
    F -->|10-30%| H[MM-Estimador]
    F -->|> 30%| I[MCD o S-Estimador]
    F -->|Desconocida| J[MCD por defecto]
```

## Validación y Diagnóstico

### Métricas de Evaluación

**1. Distancia de Procrustes:**
$$d_P(\Sigma_1, \Sigma_2) = \|\Sigma_1^{1/2} - R\Sigma_2^{1/2}\|_F$$

**2. Ángulo entre Subespacios:**
$$\theta = \arccos\left(\frac{\text{tr}(P_1^T P_2)}{\min(r_1, r_2)}\right)$$

**3. Error de Reconstrucción:**
$$MSE = \frac{1}{n}\sum_{k=1}^n \|x_k - \hat{x}_k\|^2$$

### Diagnósticos de Robustez

**1. Gráfico de Distancias:**
- Eje X: Distancias clásicas de Mahalanobis
- Eje Y: Distancias robustas de Mahalanobis
- Interpretación: Puntos alejados de la diagonal son outliers

**2. Análisis de Influencia:**
$$IF_k = \|\theta(X) - \theta(X \setminus \{x_k\})\|$$

**3. Estabilidad Bootstrap:**
```
Para b = 1 to B:
  1. Muestra bootstrap: X_b* = muestra con reemplazo de X
  2. Estimar: θ_b* = estimador_robusto(X_b*)
  3. Acumular para intervalos de confianza
```

# De la teoría al paso a paso

Esta guía resume cómo implementar GWPCA robusto desde cero, conectando cada decisión con su base teórica.

1. Preparación y estandarización
   - Teoría: PCA depende de la escala; estandarizar evita que una variable domine por unidades.
   - Práctica: escalar variables (z-score) global o localmente si hay fuertes gradientes.

2. Elección del esquema espacial
   - Teoría: la matriz de pesos define la vecindad y controla varianza–sesgo.
   - Práctica: kernel gaussiano o bisquare; bandwidth fijo o adaptativo (k vecinos). Validar con CV local/global.

3. Selección del estimador robusto
   - Teoría: compromiso entre punto de ruptura y eficiencia.
   - Práctica: MCD/MM para alta robustez; Huber si la contaminación es leve; ROBPCA si p es grande o p > n.

4. Cálculo de pesos totales
   - Teoría: combinar peso espacial y de robustez reduce influencia de outliers (IF acotada) respetando proximidad.
   - Práctica: w_total = w_espacial × w_rob (de LOF, profundidad o ψ/ρ).

5. Centro y covarianza locales robustos
   - Teoría: estimadores equivariantes afín y de alto breakdown (MCD/S/MM) mantienen estructura bajo contaminación.
   - Práctica: resolver por ventana; regularizar Σ si hay colinealidad o p cercano a n (añadir τI con τ pequeño).

6. Descomposición espectral
   - Teoría: eigenvalores/eigenvectores de Σ(s) dan varianzas y direcciones principales locales.
   - Práctica: ordenar por varianza explicada; seleccionar k por umbral (p. ej., 80–95%) o criterios de información.

7. Scores y mapas
   - Teoría: proyectar datos centrados sobre loadings locales.
   - Práctica: mapear varianza explicada, loadings y scores para interpretar patrones espaciales.

8. Diagnóstico de robustez
   - Teoría: comparar distancias clásica vs. robusta; usar IF y bootstrap para estabilidad.
   - Práctica: gráficos de distancias, influencia leave-one-out, estabilidad de loadings por re-muestreo.

9. Sensibilidad de hiperparámetros
   - Teoría: c (Huber/Tukey), h (MCD), k (LOF), bandwidth.
   - Práctica: analizar rejilla pequeña y elegir por CV y estabilidad de mapas.

10. Informe e interpretación
   - Teoría: los componentes son combinaciones lineales; su significado depende de cargas/variables dominantes.
   - Práctica: acompañar mapas con tablas de loadings locales (top-3 por zona) y bandas de confianza por bootstrap.

# Ejemplo de Código Integrado

## Pseudocódigo del GWPCA Robusto Completo

```python
function GWPCA_Robusto(X, S, metodo, bandwidth, n_componentes):
    # Inicialización
    n, p = dimensiones(X)
    resultados = []
    
    # Para cada ubicación focal
    para i = 1 hasta n:
        # Calcular pesos espaciales
        W = calcular_pesos_kernel(S, S[i], bandwidth)
        
        # Aplicar método robusto seleccionado
        switch metodo:
            case "Huber":
                mu, Sigma = huber_estimator(X, W)
            case "MCD":
                mu, Sigma = fast_mcd(X, W)
            case "MM":
                mu, Sigma = mm_estimator(X, W)
            case "ROBPCA":
                mu, Sigma = robpca(X, W)
            # ... otros métodos
        
        # Regularización
        Sigma = regularizar_matriz(Sigma)
        
        # Descomposición espectral
        valores, vectores = eigen(Sigma)
        
        # Ordenar por varianza explicada
        idx = ordenar_descendente(valores)
        valores = valores[idx[1:n_componentes]]
        vectores = vectores[:, idx[1:n_componentes]]
        
        # Calcular scores locales
        X_centrado = X - mu
        scores = X_centrado @ vectores
        
        # Almacenar resultados
        resultados[i] = {
            "ubicacion": S[i],
            "media": mu,
            "covarianza": Sigma,
            "valores_propios": valores,
            "vectores_propios": vectores,
            "scores": scores,
            "varianza_explicada": valores / suma(valores)
        }
    
    return resultados
```

# Conclusiones y Perspectivas

## Ventajas del GWPCA Robusto

1. **Resistencia a Outliers**: Mantiene estimaciones estables incluso con 30-50% de contaminación
2. **Adaptabilidad Espacial**: Diferentes niveles de robustez según el contexto local
3. **Interpretabilidad**: Los componentes mantienen significado geográfico

## Limitaciones y Desafíos

1. **Costo Computacional**: Mayor que GWPCA clásico (factor 2-10×)
2. **Selección de Parámetros**: Ancho de banda, método robusto, parámetros de tuning
3. **Maldición de la Dimensionalidad**: Degradación con p grande

## Direcciones Futuras

1. **Métodos Híbridos**: Combinar múltiples estimadores según características locales
2. **Robustez Adaptativa**: Ajustar nivel de robustez según contaminación local detectada
3. **Paralelización**: Implementación en GPU para datasets masivos
4. **Extensiones No-lineales**: GWPCA robusto con kernels no-lineales

# Referencias Bibliográficas

Billor, N., Hadi, A. S., & Velleman, P. F. (2000). BACON: blocked adaptive computationally efficient outlier nominators. *Computational Statistics & Data Analysis*, 34(3), 279-298.

Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000). LOF: identifying density-based local outliers. In *Proceedings of the 2000 ACM SIGMOD international conference on Management of data* (pp. 93-104).

Fotheringham, A. S., Brunsdon, C., & Charlton, M. (2002). *Geographically weighted regression: the analysis of spatially varying relationships*. John Wiley & Sons.

Harris, P., Brunsdon, C., & Charlton, M. (2011). Geographically weighted principal components analysis. *International Journal of Geographical Information Science*, 25(10), 1717-1736.

Huber, P. J. (1964). Robust estimation of a location parameter. *The Annals of Mathematical Statistics*, 35(1), 73-101.

Hubert, M., Rousseeuw, P. J., & Vanden Branden, K. (2005). ROBPCA: a new approach to robust principal component analysis. *Technometrics*, 47(1), 64-79.

Maronna, R. A. (1976). Robust M-estimators of multivariate location and scatter. *The Annals of Statistics*, 4(1), 51-67.

Maronna, R. A., Martin, R. D., Yohai, V. J., & Salibián-Barrera, M. (2019). *Robust statistics: theory and methods (with R)*. John Wiley & Sons.

Rousseeuw, P. J. (1984). Least median of squares regression. *Journal of the American Statistical Association*, 79(388), 871-880.

Rousseeuw, P. J. (1985). Multivariate estimation with high breakdown point. In *Mathematical statistics and applications* (pp. 283-297). Springer.

Rousseeuw, P. J., & Van Driessen, K. (1999). A fast algorithm for the minimum covariance determinant estimator. *Technometrics*, 41(3), 212-223.

Rousseeuw, P. J., & Yohai, V. (1984). Robust regression by means of S-estimators. In *Robust and nonlinear time series analysis* (pp. 256-272). Springer.

Ruppert, D. (1992). Computing S estimators for regression and multivariate location/dispersion. *Journal of Computational and Graphical Statistics*, 1(3), 253-270.

Vardi, Y., & Zhang, C. H. (2000). The multivariate L1-median and associated data depth. *Proceedings of the National Academy of Sciences*, 97(4), 1423-1426.

Yohai, V. J. (1987). High breakdown-point and high efficiency robust estimates for regression. *The Annals of Statistics*, 15(2), 642-656.
