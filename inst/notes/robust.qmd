---
title: "GWPCA Robusto: Fundamentos Matemáticos y Extensiones"
subtitle: "Detalle de los estimadores implementados en `gwmvt`"
format: html
editor: source
---

Este documento detalla la base matemática utilizada en el paquete `gwmvt` para extender el Análisis de Componentes Principales Geográficamente Ponderado (GWPCA) clásico mediante el uso de estimadores robustos. El objetivo principal es reemplazar las estimaciones locales de media y covarianza, que son sensibles a valores atípicos, por alternativas que ofrezcan estabilidad sin perder la estructura de ponderación geográfica.

## 1. Recordatorio del GWPCA Estándar

El GWPCA estándar (Harris et al., 2011) adapta el PCA clásico al contexto espacial. Para un conjunto de datos multivariados $x_k \in \mathbb{R}^p$ observados en ubicaciones $s_k \in \mathbb{R}^2$ (con $k = 1, \dots, n$), el GWPCA calcula una matriz de covarianza local para cada ubicación de interés $s_i$.

El proceso es el siguiente:

1.  **Ponderación Geográfica**: Se asigna un peso $w_k(s_i)$ a cada observación $x_k$ en función de su distancia a $s_i$. Estos pesos se calculan mediante una función kernel (ej. Gaussiano).

    $$
    w_k(s_i) = \exp\left(-\frac{d(s_i, s_k)^2}{2h^2}\right)
    $$
    Donde $h$ es el ancho de banda (bandwidth) del kernel.

2.  **Estimación Local Ponderada**: Se calculan la media y la matriz de covarianza ponderadas, que son los estimadores de máxima verosimilitud si los datos siguieran una distribución normal ponderada.

    * **Media Local Ponderada**:
        $$
        \bar{x}(s_i) = \frac{\sum_{k=1}^{n} w_k(s_i)x_{k}}{\sum_{k=1}^{n} w_k(s_i)}
        $$

    * **Matriz de Covarianza Local Ponderada**:
        $$
        \Sigma(s_i) = \frac{\sum_{k=1}^{n} w_k(s_i) (x_k - \bar{x}(s_i))(x_k - \bar{x}(s_i))^T}{\sum_{k=1}^{n} w_k(s_i)}
        $$

3.  **Extracción de Componentes**: Se realiza una descomposición de valores propios (autovalores y autovectores) sobre $\Sigma(s_i)$ para obtener los componentes principales locales.

    $$
    \Sigma(s_i) \phi_m(s_i) = \lambda_m(s_i) \phi_m(s_i)
    $$
    Donde $\lambda_m(s_i)$ es el $m$-ésimo autovalor (varianza del componente) y $\phi_m(s_i)$ es el $m$-ésimo autovector (los *loadings* del componente).

El problema de este enfoque es que si una observación $x_k$ (incluso con un peso $w_k(s_i)$ pequeño) es un valor atípico, puede distorsionar (inflar o sesgar) $\bar{x}(s_i)$ y $\Sigma(s_i)$, afectando todos los componentes principales.

## 2. La Filosofía del GWPCA Robusto

El GWPCA robusto sustituye los estimadores locales de media y covarianza (Ecuaciones 2 y 3) por alternativas robustas. La idea es encontrar una función de estimación robusta $\mathcal{R}$ que tome tanto los datos como los pesos geográficos para producir estimaciones locales robustas:

$$
(\tilde{\bar{x}}(s_i), \tilde{\Sigma}(s_i)) = \mathcal{R} \Bigl(\bigl\{ (x_k, w_k(s_i)) \bigr\}_{k=1}^n \Bigr)
$$

Una vez obtenida la matriz de covarianza robusta $\tilde{\Sigma}(s_i)$, el paso 3 (descomposición de valores propios) se realiza de la misma manera.

A continuación, se describen los estimadores $\mathcal{R}$ implementados.

## 3. Estimadores Robustos Implementados

### 3.1. M-Estimador de Huber (Maronna, 1976)

* **Concepto**: Limita la influencia de los outliers usando una función de pérdida $\rho$ que es cuadrática para observaciones "buenas" (como el GWPCA estándar) pero lineal para observaciones "malas" (outliers).

* **Fundamento Matemático**: La función de pérdida de Huber es:
    $$
    \rho_c(u) =
    \begin{cases}
    \frac{1}{2}u^2, & |u| \le c, \\
    c |u| - \frac{1}{2} c^2, & |u| > c.
    \end{cases}
    $$
    Donde $u$ es una medida de "lejanía" (como una distancia de Mahalanobis) y $c$ es un umbral de robustez. El estimador busca $(\tilde{\mu}, \tilde{\Sigma})$ que resuelva un sistema de ecuaciones implícitas basado en la derivada $\psi(u) = \rho'(u)$.

* **Algoritmo (IRWLS Geográficamente Ponderado)**: Las estimaciones se encuentran mediante un algoritmo de Mínimos Cuadrados Re-ponderados Iterativamente (IRWLS), adaptado para incluir los pesos geográficos.

    1.  **Inicialización (Paso 0)**: Se obtienen estimaciones iniciales robustas $\tilde{\mu}^{(0)}$ y $\tilde{\Sigma}^{(0)}$. Usualmente, son la media ponderada y covarianza ponderada de un subconjunto de los datos.
    2.  **Iteración (Paso t)**:
        a.  **Calcular Distancias**: Dadas las estimaciones $\tilde{\mu}^{(t-1)}$ y $\tilde{\Sigma}^{(t-1)}$, se calcula la distancia de Mahalanobis robusta $d_k$ para cada punto $x_k$:
            $$
            d_k = \sqrt{(x_k - \tilde{\mu}^{(t-1)})^T (\tilde{\Sigma}^{(t-1)})^{-1} (x_k - \tilde{\mu}^{(t-1)})}
            $$
        b.  **Calcular Pesos de Robustez**: Se calcula un *peso de robustez* $w_{rob,k}$ para cada punto, basado en la función $\psi$ de Huber. Un peso común (derivado de Maronna, 1976) es $u(d_k) = \psi(d_k) / d_k$. Este peso es 1 para puntos cercanos y disminuye para puntos lejanos (outliers).
            $$
            w_{rob,k} = u(d_k)
            $$
        c.  **Calcular Peso Total**: Se combina el peso geográfico $w_k(s_i)$ con el peso de robustez $w_{rob,k}$:
            $$
            W_k = w_k(s_i) \times w_{rob,k}
            $$
        d.  **Actualizar Estimaciones**: Se calculan la nueva media y covarianza ponderadas usando este peso total $W_k$:
            $$
            \tilde{\mu}^{(t)} = \frac{\sum_{k=1}^{n} W_k x_{k}}{\sum_{k=1}^{n} W_k}
            $$
            $$
            \tilde{\Sigma}^{(t)} = \frac{\sum_{k=1}^{n} W_k (x_k - \tilde{\mu}^{(t)})(x_k - \tilde{\mu}^{(t)})^T}{\sum_{k=1}^{n} W_k}
            $$
    3.  **Convergencia (Paso t+1)**: Se repite el Paso 2 hasta que las estimaciones $\tilde{\mu}^{(t)}$ y $\tilde{\Sigma}^{(t)}$ dejen de cambiar significativamente. Las estimaciones finales son $(\tilde{\bar{x}}(s_i), \tilde{\Sigma}(s_i))$.

### 3.2. Determinante de Covarianza Mínima (MCD)

* **Concepto**: (Rousseeuw, 1984). Estimador de alto punto de ruptura. Su lógica es que si al menos $h$ de $n$ puntos son "buenos", debemos encontrar ese subconjunto $H$ de $h$ puntos y calcular la covarianza *solo* de ellos. El "mejor" subconjunto es aquel cuya covarianza clásica tenga el menor determinante posible (el elipsoide más pequeño).

* **Fundamento Matemático**: Se busca un subconjunto $H \subset \{1, \dots, n\}$ con $|H|=h$ (usualmente $h \approx 0.75n$) que minimice $\det(\Sigma_H)$, donde $\Sigma_H$ es la covarianza clásica del subconjunto $H$.

* **Algoritmo (Fast-MCD Ponderado)**: (Rousseeuw & Van Driessen, 1999). El algoritmo "Fast-MCD" utiliza "C-Pasos" (Pasos de Concentración) para encontrar este subconjunto $H$. Lo adaptamos para GWPCA.

    1.  **Inicialización (Paso 0)**: Se generan muchos subconjuntos aleatorios $J$ de $p+1$ puntos. Para cada $J$, se calcula su media $\bar{x}_J$ y covarianza $\Sigma_J$ *ponderadas* (usando $w_k(s_i)$).
    2.  **C-Pasos (Iteración)**: Se toma el mejor subconjunto inicial $H^{(0)}$ (el que dio el $\det(\Sigma_J)$ más bajo).
        a.  **Estimar**: Se calcula la media $\tilde{\mu}^{(t)}$ y covarianza $\tilde{\Sigma}^{(t)}$ *ponderadas* (con $w_k(s_i)$) usando *solo* los puntos del subconjunto $H^{(t)}$.
        b.  **Distancias**: Se calculan las distancias de Mahalanobis $d_k$ para *todos* los $n$ puntos usando $\tilde{\mu}^{(t)}$ y $\tilde{\Sigma}^{(t)}$.
        c.  **Concentrar**: Se crea un nuevo subconjunto $H^{(t+1)}$ seleccionando los $h$ puntos que tengan las $d_k$ más pequeñas.
        d.  Se repiten (a)-(c) hasta que $H$ no cambie.
    3.  **Estimación Final (Re-ponderación)**: El MCD crudo (del paso 2) no es muy eficiente. Se realiza un paso final de re-ponderación:
        a.  Se obtienen $(\tilde{\mu}_{MCD}, \tilde{\Sigma}_{MCD})$ del $H$ final.
        b.  Se calculan las distancias robustas $d_R(k)$ para todos los puntos.
        c.  Se asigna un peso de robustez $w_{rob,k} = 1$ si $d_R(k)^2 \le \chi^2_{p, 0.975}$ y $w_{rob,k} = 0$ si es un outlier.
        d.  Las estimaciones finales $(\tilde{\bar{x}}(s_i), \tilde{\Sigma}(s_i))$ son la media y covarianza ponderadas usando el peso total $W_k = w_k(s_i) \times w_{rob,k}$.

### 3.3. Elipsoide de Volumen Mínimo (MVE)

* **Concepto**: (Rousseeuw, 1985). Similar al MCD, pero busca el elipsoide de mínimo volumen (no determinante de covarianza) que contenga al menos $h$ de las observaciones. El centro y la forma de este elipsoide definen la media y covarianza robustas.

* **Algoritmo (Ponderado)**: Generalmente se implementa mediante sub-muestreo aleatorio.

    1.  **Muestreo (Paso 1)**: Se repite $M$ veces:
        a.  Se elige un subconjunto aleatorio $J$ de $p+1$ puntos.
        b.  Se calcula su media $\bar{x}_J$ y covarianza $\Sigma_J$ *ponderadas* (usando $w_k(s_i)$).
        c.  Se calculan las distancias de Mahalanobis $d_k$ para todos los puntos basadas en $(\bar{x}_J, \Sigma_J)$.
        d.  Se encuentra el cuantil $q$ de las distancias (el $h$-ésimo valor más pequeño).
        e.  Se "infla" el elipsoide $(\bar{x}_J, \Sigma_J)$ por $q^2$ para que contenga $h$ puntos. El volumen de este elipsoide es proporcional a $\det(\Sigma_J) \times q^{2p}$.
    2.  **Selección (Paso 2)**: Se guardan la media $\tilde{\mu}_{MVE}$ y la covarianza $\tilde{\Sigma}_{MVE}$ del subconjunto que produjo el volumen mínimo en el Paso 1.
    3.  **Estimación Final**: Al igual que con MCD, se suele aplicar un paso de re-ponderación (como en 3.2, Paso 3) usando $(\tilde{\mu}_{MVE}, \tilde{\Sigma}_{MVE})$ para obtener las estimaciones finales $(\tilde{\bar{x}}(s_i), \tilde{\Sigma}(s_i))$.

### 3.4. S-Estimadores

* **Concepto**: (Rousseeuw & Yohai, 1984). Son estimadores de alto punto de ruptura que minimizan una M-estimación de la *escala* de los residuos (distancias de Mahalanobis).

* **Fundamento Matemático**: Buscan $(\tilde{\mu}, \tilde{\Sigma})$ que minimicen la escala $s$, definida implícitamente por la ecuación ponderada:
    $$
    \frac{1}{\sum_{j=1}^n w_j(s_i)} \sum_{k=1}^n w_k(s_i) \rho\left(\frac{d_k}{s}\right) = K
    $$
    Donde $d_k$ son las distancias de Mahalanobis basadas en $(\tilde{\mu}, \tilde{\Sigma})$, $\rho$ es una función de pérdida (ej. biponderada de Tukey, que da peso 0 a outliers extremos) y $K$ es una constante para asegurar consistencia.

* **Algoritmo (Iterativo Ponderado)**: Es un algoritmo complejo que anida dos bucles de iteración: uno interno para encontrar la escala $s$ y uno externo para actualizar $(\tilde{\mu}, \tilde{\Sigma})$.
    1.  **Inicialización**: Se parte de una estimación robusta inicial (ej. MCD ponderado), $(\tilde{\mu}^{(0)}, \tilde{\Sigma}^{(0)})$.
    2.  **Iteración (Paso t)**:
        a.  **Calcular Distancias**: $d_k = d(x_k, \tilde{\mu}^{(t-1)}, \tilde{\Sigma}^{(t-1)})$.
        b.  **Resolver Escala**: Se encuentra la escala $s^{(t)}$ que resuelve la ecuación de escala ponderada (ver arriba) para las $d_k$ fijas.
        c.  **Pesos de Robustez**: Se calculan los pesos de robustez $w_{rob,k} = u(d_k/s^{(t)})$, donde $u(v) = \psi(v)/v$.
        d.  **Peso Total**: $W_k = w_k(s_i) \times w_{rob,k}$.
        e.  **Actualizar**: Se actualizan $(\tilde{\mu}^{(t)}, \tilde{\Sigma}^{(t)})$ usando $W_k$ (como en el algoritmo de Huber, Paso 2d).
    3.  **Convergencia**: Se repite hasta que $(\tilde{\mu}^{(t)}, \tilde{\Sigma}^{(t)})$ converjan.

### 3.5. MM-Estimadores

* **Concepto**: (Yohai, 1987). Combinan la alta robustez de los S-estimadores con la alta eficiencia (cercanía al estimador estándar si no hay outliers) de los M-estimadores.

* **Algoritmo (Ponderado de dos etapas)**:
    1.  **Etapa 1: Estimación S (Robusta)**:
        a.  Se calcula un S-estimador $(\tilde{\mu}_S, \tilde{\Sigma}_S)$ como se describe en la sección 3.4, usando los pesos $w_k(s_i)$.
        b.  Se obtiene la escala final $s_S$ de este estimador.
    2.  **Etapa 2: Estimación M (Eficiente)**:
        a.  Se *fijan* la escala $s_S$ y la estructura de covarianza $\tilde{\Sigma}_S$.
        b.  Se utiliza $\tilde{\mu}_S$ como punto de partida para una M-estimación (como la de Huber, pero con una función $\rho$ diferente, diseñada para alta eficiencia).
        c.  Se ejecuta un algoritmo IRWLS (similar a 3.1) donde los pesos de robustez $w_{rob,k}$ se calculan usando las distancias $d_k$ (basadas en $\tilde{\Sigma}_S$) y la escala fija $s_S$.
        d.  El resultado es la estimación final $(\tilde{\bar{x}}(s_i), \tilde{\Sigma}(s_i))$, que tiene la misma robustez que el S-estimador pero mejor eficiencia.

### 3.6. BACON (Blocked Adaptive Computationally-efficient Outlier Nominators)

* **Concepto**: (Billor et al., 2000). Es un algoritmo iterativo que identifica un subconjunto "limpio" haciéndolo crecer desde un "núcleo" inicial garantizado de puntos no atípicos.

* **Algoritmo (Ponderado)**:
    1.  **Núcleo Inicial (Paso 0)**: Se identifica un subconjunto $H^{(0)}$ de puntos que se consideran "limpios". Esto se puede hacer, por ejemplo, tomando los puntos más cercanos a la mediana ponderada (usando $w_k(s_i)$).
    2.  **Iteración (Paso t)**:
        a.  **Estimar**: Se calcula la media $\tilde{\mu}^{(t)}$ y covarianza $\tilde{\Sigma}^{(t)}$ *ponderadas* (con $w_k(s_i)$) usando *solo* los puntos del subconjunto $H^{(t)}$.
        b.  **Distancias**: Se calculan las distancias de Mahalanobis $d_k$ para *todos* los $n$ puntos.
        c.  **Crecer**: Se crea un nuevo subconjunto $H^{(t+1)}$ que incluye todos los puntos $k$ cuya distancia $d_k$ esté por debajo de un umbral (ej. $d_k^2 \le \chi^2_{p, 1-\alpha/n}$).
    3.  **Convergencia**: Se repite el Paso 2 hasta que $H^{(t+1)} = H^{(t)}$.
    4.  **Estimación Final**: Las estimaciones $(\tilde{\bar{x}}(s_i), \tilde{\Sigma}(s_i))$ son la media y covarianza ponderadas (usando $w_k(s_i)$) del subconjunto limpio final $H$.

### 3.7. Ponderación por Factor Local de Outlier (LOF)

* **Concepto**: (Breunig et al., 2000). Este no es un estimador de covarianza en sí mismo, sino un método para detectar outliers basado en la densidad local. Un punto con LOF alto está en una región de densidad mucho menor que la de sus vecinos (es un outlier local).

* **Fundamento Matemático**: El LOF de un punto $x_k$ se define como el promedio de la relación entre la densidad local de sus vecinos y su propia densidad local.
    $$
    LOF(x_k) = \frac{1}{|N_j|} \sum_{j \in N_k} \frac{lrd(x_j)}{lrd(x_k)}
    $$
    Donde $N_k$ es el conjunto de $j$ vecinos de $x_k$ y $lrd$ es la *densidad de alcanzabilidad local* (inversa de la distancia promedio a los vecinos). Un $LOF \approx 1$ significa densidad similar a los vecinos; $LOF \gg 1$ significa que es un outlier.

* **Algoritmo (Re-ponderación GWPCA)**: Usamos el LOF para definir un peso de robustez.
    1.  **Cálculo de LOF (Paso 1)**: Se calcula el valor $LOF(x_k)$ para cada punto $x_k$ en el conjunto de datos. Este cálculo se hace *antes* del GWPCA y no depende de la ubicación $s_i$.
    2.  **Peso de Robustez (Paso 2)**: Se define un peso de robustez $w_{rob,k}$. Una opción común es:
        $$
        w_{rob,k} = 1 / LOF(x_k)
        $$
        (A veces se acota, ej. $w_{rob,k} = \min(1, 1/LOF(x_k))$).
    3.  **Estimación Final (Paso 3)**: Se calcula un peso total $W_k = w_k(s_i) \times w_{rob,k}$. Las estimaciones finales $(\tilde{\bar{x}}(s_i), \tilde{\Sigma}(s_i))$ son la media y covarianza ponderadas estándar (Ecuaciones 2 y 3) usando $W_k$.

### 3.8. ROBPCA

* **Concepto**: (Hubert et al., 2005). Combina la robustez de MCD con una técnica de reducción de dimensionalidad (Proyección-Persecución). Es ideal para datos de alta dimensionalidad ($p$ grande). La idea es que los outliers "malos" inflan la covarianza, mientras que los outliers "buenos" (apalancamiento) pueden definir componentes principales.

* **Algoritmo (Adaptación Ponderada)**:
    1.  **Pre-procesamiento (Paso 1)**: Se centra la data robustamente. Se calcula una M-estimación ponderada (con $w_k(s_i)$) de la media, $\tilde{\mu}_{rob}$, y se resta: $x'_k = x_k - \tilde{\mu}_{rob}$.
    2.  **Subespacio Robusto (Paso 2)**: Se buscan las $q$ (con $q < p$) direcciones (vectores $v_j$) que maximizan la varianza robusta de los datos proyectados. Esto se hace con un algoritmo de Proyección-Persecución (PP) que utiliza una escala robusta (como el MAD) sobre las proyecciones $x'_k \cdot v_j$, ponderadas por $w_k(s_i)$.
    3.  **Proyección (Paso 3)**: Se proyectan los datos $x'_k$ al subespacio robusto $q$-dimensional definido por los $q$ vectores $V = [v_1, \dots, v_q]$, obteniendo $z_k = V^T x'_k$.
    4.  **MCD Ponderado (Paso 4)**: Se aplica el algoritmo Fast-MCD Ponderado (ver 3.2) al conjunto de datos proyectados $\{ (z_k, w_k(s_i)) \}_{k=1}^n$ para obtener una media $\tilde{\mu}_z$ y covarianza $\tilde{\Sigma}_z$ robustas en el subespacio $\mathbb{R}^q$.
    5.  **Retro-proyección (Paso 5)**: Se retro-proyecta la covarianza robusta al espacio original $\mathbb{R}^p$:
        $$
        \tilde{\Sigma}(s_i) = V \tilde{\Sigma}_z V^T
        $$
    6.  El centro $\tilde{\bar{x}}(s_i)$ es $\tilde{\mu}_{rob} + V \tilde{\mu}_z$.

### 3.9. Estimador Basado en Profundidad Espacial (Spatial Depth)

* **Concepto**: La *profundidad espacial* (Vardi & Zhang, 2000) de un punto $y$ mide su centralidad respecto a la nube de puntos. El punto más central (la mediana espacial) tiene la máxima profundidad, mientras que los outliers tienen profundidad cercana a 0.

* **Fundamento Matemático**: La profundidad espacial de un punto $y$ respecto a un conjunto de datos $X = \{x_k\}_{k=1}^n$ es:
    $$
    SD(y, X) = 1 - \left\| \frac{1}{n} \sum_{k=1}^n \frac{y - x_k}{\|y - x_k\|} \right\|
    $$
    Donde $\frac{y - x_k}{\|y - x_k\|}$ es el vector unitario que va de $x_k$ a $y$. Si $y$ es central, la suma de estos vectores unitarios tiende a cancelarse (norma cercana a 0), dando una profundidad cercana a 1.

* **Algoritmo (Re-ponderación GWPCA)**: Similar al LOF, usamos la profundidad como un peso de robustez.
    1.  **Cálculo de Profundidad (Paso 1)**: Se calcula la profundidad $d_k = SD(x_k, X)$ para cada punto $x_k$ (usando la fórmula estándar, no ponderada).
    2.  **Peso de Robustez (Paso 2)**: Se usa la profundidad directamente como peso de robustez: $w_{rob,k} = d_k$. Los puntos centrales (alta $d_k$) reciben más peso que los outliers (baja $d_k$).
    3.  **Estimación Final (Paso 3)**: Se calcula el peso total $W_k = w_k(s_i) \times w_{rob,k}$. Las estimaciones finales $(\tilde{\bar{x}}(s_i), \tilde{\Sigma}(s_i))$ son la media y covarianza ponderadas estándar (Ecuaciones 2 y 3) usando $W_k$.

### 3.10. Recorte Espacial (Spatial Trimming)

* **Concepto**: (Maronna et al., 2019). Es un método general que consiste en "recortar" (eliminar) las observaciones que se identifican como outliers *antes* de calcular un estimador clásico.

* **Algoritmo (Ponderado)**:
    1.  **Estimación Inicial (Paso 1)**: Se calcula una estimación inicial robusta de media y covarianza, $(\tilde{\mu}_{rob}, \tilde{\Sigma}_{rob})$, que sea de alto punto de ruptura. Es crucial que esta estimación inicial ya incorpore los pesos geográficos, por ejemplo, usando el **MCD Ponderado (3.2)** o un **S-Estimador Ponderado (3.4)**.
    2.  **Distancias Robustas (Paso 2)**: Se calculan las distancias de Mahalanobis robustas $d_R(k)$ para todos los puntos, usando $(\tilde{\mu}_{rob}, \tilde{\Sigma}_{rob})$.
    3.  **Identificar Outliers (Paso 3)**: Se define un umbral basado en la distribución Chi-cuadrado. Se crea un conjunto "limpio" $H$ (o un peso de robustez) para los puntos que no son outliers.
        $$
        H = \{k \mid d_R(k)^2 \le \chi^2_{p, 1-\alpha}\}
        $$
        O, equivalentemente, $w_{rob,k} = 1$ si $k \in H$ y $w_{rob,k} = 0$ si $k \notin H$.
    4.  **Estimación Final (Paso 4)**: Se calcula el peso total $W_k = w_k(s_i) \times w_{rob,k}$. (Esto efectivamente aplica los pesos geográficos $w_k(s_i)$ solo a los puntos del subconjunto limpio $H$).
    5.  Las estimaciones finales $(\tilde{\bar{x}}(s_i), \tilde{\Sigma}(s_i))$ son la media y covarianza ponderadas estándar (Ecuaciones 2 y 3) usando $W_k$.

## 4. Regularización y Estabilidad Numérica

Independientemente del estimador, la matriz $\tilde{\Sigma}(s_i)$ resultante debe ser simétrica y definida positiva para poder calcular sus autovectores. El código implementa salvaguardas como la **simetrización** ($\tilde{\Sigma} \leftarrow \frac{1}{2}(\tilde{\Sigma} + \tilde{\Sigma}^T)$) y la **regularización Ridge** (sumar una pequeña constante $\lambda \mathbf{I}$ a la diagonal) para asegurar la estabilidad numérica antes de la descomposición de valores propios.

## 5. Referencias Bibliográficas

-   Billor, N., Hadi, A. S., & Velleman, P. F. (2000). BACON: Blocked Adaptive Computationally-efficient Outlier Nominators. *Computational Statistics & Data Analysis*, 34(3), 279-298.
-   Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000). LOF: identifying density-based local outliers. In *Proceedings of the 2000 ACM SIGMOD international conference on Management of data* (pp. 93-104).
-   Fotheringham, A. S., Brunsdon, C., & Charlton, M. (2002). *Geographically weighted regression: the analysis of spatially varying relationships*. John Wiley & Sons.
-   Harris, P., Clarke, A., Juggins, S., Brunsdon, C., & Charlton, M. (2011). Geographically weighted principal components analysis. *Geographical and Environmental Modelling*, 15(1), 29-52.
-   Huber, P. J. (1964). Robust Estimation of a Location Parameter. *The Annals of Mathematical Statistics*, 35(1), 73-101.
-   Hubert, M., Rousseeuw, P. J., & Vanden Branden, K. (2005). ROBPCA: a new approach to robust principal component analysis. *Technometrics*, 47(1), 1-12.
-   Maronna, R. A. (1976). Robust M-estimators of multivariate location and scatter. *The Annals of Statistics*, 4(1), 51-67.
-   Maronna, R. A., Martin, R. D., & Yohai, V. J. (2019). *Robust statistics: theory and methods (with R)*. John Wiley & Sons.
-   Rousseeuw, P. J. (1984). Least Median of Squares Regression. *Journal of the American Statistical Association*, 79(388), 871-880.
-   Rousseeuw, P. J. (1985). Multivariate Estimation with High Breakdown Point. In *Mathematical Statistics and Applications* (pp. 283-297). Springer, Dordrecht.
-   Rousseeuw, P. J., & Van Driessen, K. (1999). A fast algorithm for the minimum covariance determinant estimator. *Technometrics*, 41(3), 212-223.
-   Rousseeuw, P. J., & Yohai, V. J. (1984). Robust regression by means of S-estimators. In *Robust and nonlinear time series analysis* (pp. 256-272). Springer, New York, NY.
-   Vardi, Y., & Zhang, C. H. (2000). The multivariate L1-median and associated data depth. *Proceedings of the National Academy of Sciences*, 97(4), 1423-1426.
-   Yohai, V. J. (1987). High breakdown-point and high efficiency robust estimates for regression. *The Annals of Statistics*, 15(2), 642-656.